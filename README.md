# question_generator

QA generator is an NLP system for generating reading comprehension-style questions from texts such as news articles or pages excerpts from books. The system is built using pretrained models from [HuggingFace Transformers](https://github.com/huggingface/transformers). 
There are 3 models: the QA generator, QA MCQ generator, and the distractor generator.

### Updated training scripts

To get model fine-tuned with custom dataset, Run:

```bash
!python training/train_qa.py
```

```bash
!python training/train_qa_mcq.py
```

```bash
!python training/train_distractors.py
```


Hyperparameters can be changed using arguments. See the scripts for the list of available arguments.

### Datasets custom
Public free on github for all who loves NLP: https://github.com/TrongNV2003/T5-QA-Generator/tree/main/datasets

## Usage

Then, to generate QA you have to clone the github repo and fine-tune 3 model above, then run `!python run_qa.py`, for example:

```
git clone https://github.com/TrongNV2003/T5-QA-Generator
cd T5-QA-Generator
pip install -r requirements.txt -qq
!python run_qa.py
```

This will generate 5 question-answer pairs (full-sentence or multiple choice) based on the article specified in `--text_file` and print them to the console.

The `QuestionGenerator` class can also be instantiated and used like this:

```python
from questiongenerator import QuestionGenerator
qg = QuestionGenerator()
qg.generate(text, num_questions=5)
```

It will generate 5 questions of mixed style and return a list of dictionaries containing question-answer pairs. In the case of multiple choice questions, the answer will contain a list of dictionaries containing the answers and a boolean value stating if the answer is correct or not. The output can be easily printed using the `print_qa()` function.

## Evaluate score
I use the pakage which is created to evaluate: https://github.com/p208p2002/nqg.git

Get scorer
```python
!python setup_scorer.py 
```

Evaluation for 2 types of QA

```python
!python nqg/qgevalcap/eval.py \
  --src datasets/score_evaluation_mcq/predict.txt \
  --tgt datasets/score_evaluation_mcq/tgt-test.txt \
  --out datasets/score_evaluation_mcq/predict.txt
```
### Answer styles

The system can generate questions with full-sentence answers (`'sentences'`), questions with multiple-choice answers (`'multiple_choice'`). This can be selected in `run_qa.py` using the `answer_style=<style>` arguments.

## Models

### Question Answer Generator
Since the 2 type questions have different format: The sentences answer often long and extract from context, the MCQ answer often short. So i fine-tune 2 type of questions into 2 independent model, but the way of fine-tuning is similar.

The question generator model takes Question-Answer-Context as input and outputs a series of question and answer pairs. The answers are sentences and phrases extracted from the input text. The extracted phrases can be either full sentences or choices. Choices are used for multiple-choice answers. The wrong answers had been trained by the DistractorGenerator model. The questions are generated by the following format (up to a maximum of 512 tokens) for 2 type of questions:

```
  {
    "context": "",
    "question_type": "sentences",
    "question": "",
    "answer": ""
  }
  {
    "context": "",
    "question_type": "multiple_choice",
    "question": "",
    "options": ["A", "B", "C", "D"],
    "answer": "A"
  }
```

Then encoded and fed into the question generator model. The model architecture is `VietAI/vit5-base`. The pretrained model was finetuned as a sequence-to-sequence model on a dataset made up by extract data from philosophy book for specific task. The datasets were restructured by concatenating the answer and context fields into the previously mentioned format. The concatenated answer and context was then used as an input for training, and the question field became the targets.

### Distractor Generator

The Distractor takes a Question - Answer - Context as an input and outputs are 3 distractors similar to correct answer. The model is `VietAI/vit5-base`. The pretrained model was finetuned on the same data as the QuestionAnswer generator model, but the output are 3 distractors. It is the following format:

```
  {
    "context": "",
    "question_type": "multiple_choice",
    "question": "",
    "options": ["A", "B", "C", "D"],
    "answer": "A"
  }
```

## UI
To have a deep inside view in this QA problem, UI is created for users to easy to use 
You have to fine-tune 3 models above first, then run:
```python
!python app.py
```